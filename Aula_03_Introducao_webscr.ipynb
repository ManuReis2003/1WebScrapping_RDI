{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9ae33d",
   "metadata": {},
   "source": [
    "# Aula 1 — Introdução prática ao Web Scraping no contexto de RI\n",
    "\n",
    "**Objetivo:** apresentar um primeiro pipeline de **coleta de dados web** com `requests` + `BeautifulSoup`, conectando cada etapa ao ciclo de **Recuperação da Informação (RI)**:\n",
    "1) **Coleta** (HTTP request e parsing do HTML);  \n",
    "2) **Representação/Indexação** (limpeza e estruturação em tabela);  \n",
    "3) **Busca** (seleção e filtragem dos elementos relevantes);  \n",
    "4) **Avaliação** (verificação de qualidade, cobertura e precisão dos dados coletados).\n",
    "\n",
    "**Resultados esperados:** ao final, a turma compreende a anatomia de uma página, seleciona elementos com segurança e produz um dataset mínimo para exploração posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umnyaMXcwbX0"
   },
   "source": [
    "# Atividade Prática: Web Scraping com Python\n",
    "RDI\n",
    "Aula 01 - Introdução - 1/2025 -10022025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6HViC4ewKOP"
   },
   "source": [
    "# Passo 1: Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baixar: pip install beautifulsoup4 -- no terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "we4x8F11vi50"
   },
   "outputs": [],
   "source": [
    "# Passo 1: Importação das bibliotecas necessárias\n",
    "import requests           # Realiza requisições HTTP para acessar páginas web\n",
    "from bs4 import BeautifulSoup  # type: ignore # Faz a análise e manipulação do conteúdo HTML/XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758b8d0",
   "metadata": {},
   "source": [
    "## Ética, legalidade e limites práticos\n",
    "\n",
    "- **Respeite `robots.txt`** e os **Termos de Uso** do site. Scraping não deve violar restrições explícitas.  \n",
    "- **LGPD:** evite coletar dados pessoais identificáveis sem base legal.  \n",
    "- **Carga no servidor:** use intervalos entre requisições e volume responsável.  \n",
    "- **Identificação do agente:** quando for adequado, configure `User-Agent` claro (ex.: fins acadêmicos).  \n",
    "- **Prefira APIs oficiais** quando existirem; scraping é “último recurso” para dados públicos não disponibilizados via API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKp98c9dxQd5"
   },
   "source": [
    "Consulte a documentação AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb19e9",
   "metadata": {},
   "source": [
    "## Anatomia da requisição e do HTML (DOM)\n",
    "\n",
    "- **HTTP Request/Response:** enviamos um pedido (URL, headers) e recebemos um **status code** (200 = OK, 403/404 = bloqueios/ausência).  \n",
    "- **HTML/DOM:** a página é uma árvore (tags, atributos, texto). O BeautifulSoup transforma esse HTML em uma estrutura navegável.  \n",
    "- **Encoding:** páginas podem ter encodes distintos; atenção a acentos e `response.encoding`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMrGMsvwwFRd"
   },
   "source": [
    "# Passo 2: Realizando uma requisição HTTP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eO18NjmxvyPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Fake Python\n",
      "  </title>\n",
      "  <link href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <section class=\"section\">\n",
      "   <div class=\"container mb-5\">\n",
      "    <h1 class=\"title is-1\">\n",
      "     Fake Python\n",
      "    </h1>\n",
      "    <p class=\"subtitle is-3\">\n",
      "     Fake Jobs for Your Web Scraping Journey\n",
      "    </p>\n",
      "   </div>\n",
      "   <div class=\"container\">\n",
      "    <div class=\"columns is-multiline\" id=\"ResultsContainer\">\n",
      "     <div class=\"column is-half\">\n",
      "      <div class=\"card\">\n",
      "       <div class=\"card-content\">\n",
      "        <div class=\"media\">\n",
      "         <div class=\"media-left\">\n",
      "          <figure class=\"image is-48x48\">\n",
      "           <img alt=\"Real Python Logo\" src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\"/>\n",
      "          </figure>\n",
      "         </div>\n",
      "         <div class=\"media-content\">\n",
      "          <h2 c\n"
     ]
    }
   ],
   "source": [
    "# URL do site a ser raspado\n",
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "# Realiza a requisição para obter o conteúdo HTML da página\n",
    "response = requests.get(url)\n",
    "\n",
    "# Cria um objeto BeautifulSoup para análise do HTML\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Exibe uma versão formatada dos primeiros 1000 caracteres do HTML\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMqp9wb2v7zW"
   },
   "source": [
    "# Passo 3: Extraindo informações da página"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b38b73",
   "metadata": {},
   "source": [
    "## Seletores (CSS) e estratégia de extração\n",
    "\n",
    "- **Seletores CSS** são diretos e legíveis (`div.card > a.title`, `.classe`, `#id`).  \n",
    "- **Estratégia robusta:** prefira seletores **estáveis** (classes/ids que não mudam a cada deploy).  \n",
    "- **Validação rápida:** antes de popular um DataFrame, teste `len(soup.select('SELETOR'))` para confirmar que o seletor encontra os elementos esperados.  \n",
    "- **Fallback:** se possível, garanta um plano B (ex.: `find()` com condições mais genéricas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M59Uh3npv3uY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título da Vaga: Senior Python Developer\n",
      "Título da Vaga: Energy engineer\n",
      "Título da Vaga: Legal executive\n",
      "Título da Vaga: Fitness centre manager\n",
      "Título da Vaga: Product manager\n",
      "Título da Vaga: Medical technical officer\n",
      "Título da Vaga: Physiological scientist\n",
      "Título da Vaga: Textile designer\n",
      "Título da Vaga: Television floor manager\n",
      "Título da Vaga: Waste management officer\n",
      "Título da Vaga: Software Engineer (Python)\n",
      "Título da Vaga: Interpreter\n",
      "Título da Vaga: Architect\n",
      "Título da Vaga: Meteorologist\n",
      "Título da Vaga: Audiological scientist\n",
      "Título da Vaga: English as a second language teacher\n",
      "Título da Vaga: Surgeon\n",
      "Título da Vaga: Equities trader\n",
      "Título da Vaga: Newspaper journalist\n",
      "Título da Vaga: Materials engineer\n",
      "Título da Vaga: Python Programmer (Entry-Level)\n",
      "Título da Vaga: Product/process development scientist\n",
      "Título da Vaga: Scientist, research (maths)\n",
      "Título da Vaga: Ecologist\n",
      "Título da Vaga: Materials engineer\n",
      "Título da Vaga: Historic buildings inspector/conservation officer\n",
      "Título da Vaga: Data scientist\n",
      "Título da Vaga: Psychiatrist\n",
      "Título da Vaga: Structural engineer\n",
      "Título da Vaga: Immigration officer\n",
      "Título da Vaga: Python Programmer (Entry-Level)\n",
      "Título da Vaga: Neurosurgeon\n",
      "Título da Vaga: Broadcast engineer\n",
      "Título da Vaga: Make\n",
      "Título da Vaga: Nurse, adult\n",
      "Título da Vaga: Air broker\n",
      "Título da Vaga: Editor, film/video\n",
      "Título da Vaga: Production assistant, radio\n",
      "Título da Vaga: Engineer, communications\n",
      "Título da Vaga: Sales executive\n",
      "Título da Vaga: Software Developer (Python)\n",
      "Título da Vaga: Futures trader\n",
      "Título da Vaga: Tour manager\n",
      "Título da Vaga: Cytogeneticist\n",
      "Título da Vaga: Designer, multimedia\n",
      "Título da Vaga: Trade union research officer\n",
      "Título da Vaga: Chemist, analytical\n",
      "Título da Vaga: Programmer, multimedia\n",
      "Título da Vaga: Engineer, broadcasting (operations)\n",
      "Título da Vaga: Teacher, primary school\n",
      "Título da Vaga: Python Developer\n",
      "Título da Vaga: Manufacturing systems engineer\n",
      "Título da Vaga: Producer, television/film/video\n",
      "Título da Vaga: Scientist, forensic\n",
      "Título da Vaga: Bonds trader\n",
      "Título da Vaga: Editorial assistant\n",
      "Título da Vaga: Photographer\n",
      "Título da Vaga: Retail banker\n",
      "Título da Vaga: Jewellery designer\n",
      "Título da Vaga: Ophthalmologist\n",
      "Título da Vaga: Back-End Web Developer (Python, Django)\n",
      "Título da Vaga: Licensed conveyancer\n",
      "Título da Vaga: Futures trader\n",
      "Título da Vaga: Counselling psychologist\n",
      "Título da Vaga: Insurance underwriter\n",
      "Título da Vaga: Engineer, automotive\n",
      "Título da Vaga: Producer, radio\n",
      "Título da Vaga: Dispensing optician\n",
      "Título da Vaga: Designer, fashion/clothing\n",
      "Título da Vaga: Chartered loss adjuster\n",
      "Título da Vaga: Back-End Web Developer (Python, Django)\n",
      "Título da Vaga: Forest/woodland manager\n",
      "Título da Vaga: Clinical cytogeneticist\n",
      "Título da Vaga: Print production planner\n",
      "Título da Vaga: Systems developer\n",
      "Título da Vaga: Graphic designer\n",
      "Título da Vaga: Writer\n",
      "Título da Vaga: Field seismologist\n",
      "Título da Vaga: Chief Strategy Officer\n",
      "Título da Vaga: Air cabin crew\n",
      "Título da Vaga: Python Programmer (Entry-Level)\n",
      "Título da Vaga: Warden/ranger\n",
      "Título da Vaga: Sports therapist\n",
      "Título da Vaga: Arts development officer\n",
      "Título da Vaga: Printmaker\n",
      "Título da Vaga: Health and safety adviser\n",
      "Título da Vaga: Manufacturing systems engineer\n",
      "Título da Vaga: Programmer, applications\n",
      "Título da Vaga: Medical physicist\n",
      "Título da Vaga: Media planner\n",
      "Título da Vaga: Software Developer (Python)\n",
      "Título da Vaga: Surveyor, land/geomatics\n",
      "Título da Vaga: Legal executive\n",
      "Título da Vaga: Librarian, academic\n",
      "Título da Vaga: Barrister\n",
      "Título da Vaga: Museum/gallery exhibitions officer\n",
      "Título da Vaga: Radiographer, diagnostic\n",
      "Título da Vaga: Database administrator\n",
      "Título da Vaga: Furniture designer\n",
      "Título da Vaga: Ship broker\n"
     ]
    }
   ],
   "source": [
    "# Localiza todos os blocos que contêm informações das vagas\n",
    "job_elements = soup.find_all('div', class_='card-content')\n",
    "\n",
    "# Itera sobre cada bloco e imprime os títulos das vagas\n",
    "for job in job_elements:\n",
    "    title = job.find('h2', class_='title').get_text().strip()\n",
    "    print(\"Título da Vaga:\", title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6bSlG69ztYD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Parte 4: Cabeçalho User-Agent e Organização dos Dados com pandas\n",
    "\n",
    "Nesta parte, é implementado um cabeçalho (User-Agent) na\n",
    "\n",
    "*   Nesta parte, é implementado um cabeçalho (User-Agent)requisição HTTP para reduzir o risco de bloqueios pelo servidor.\n",
    "\n",
    "*   Além disso, os dados extraídos são organizados em um DataFrame utilizando a biblioteca pandas e, por fim, exportados para um arquivo CSV.\n",
    "\n",
    "## Vamos estudar isto com **calma e elegância** no momento apropriado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f15f3",
   "metadata": {},
   "source": [
    "## Ligação com o ciclo de Recuperação da Informação (RI)\n",
    "\n",
    "- **Coleta (scraping):** adquirir o conteúdo bruto (HTML).  \n",
    "- **Representação/Indexação:** padronizar campos, normalizar texto (minúsculas, remoção de espaços) e estruturar em **tabela**.  \n",
    "- **Busca/Consulta:** filtrar/ranquear os itens coletados por campos (título, data, link).  \n",
    "- **Avaliação:** checar **cobertura** (quantos itens esperados foram coletados?), **precisão** (quantos itens são realmente relevantes?) e **atualidade** (dados atualizados?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f19f4",
   "metadata": {},
   "source": [
    "## Saída, versionamento e registro\n",
    "\n",
    "- **CSV com timestamp**: salvar como `dados_scraping_YYYYMMDD.csv` para rastreabilidade.  \n",
    "- **README curto (GitHub Classroom):** descreva alvo, seletor usado, data/hora, contagem de itens e limitações conhecidas.  \n",
    "- **Reprodutibilidade:** fixe versões de pacotes no `requirements.txt` (ou registre `pip freeze`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z8pcZ-ESzsgg"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m           \u001b[38;5;66;03m# Realiza requisições HTTP com cabeçalho customizado\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup  \u001b[38;5;66;03m# Faz a análise do conteúdo HTML/XML\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m       \u001b[38;5;66;03m# Manipulação e organização dos dados em DataFrame\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Definição de um cabeçalho com User-Agent para simular um navegador real\u001b[39;00m\n\u001b[32m      7\u001b[39m headers = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mMozilla/5.0 (Windows NT 10.0; Win64; x64) \u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m      9\u001b[39m                   \u001b[33m\"\u001b[39m\u001b[33mAppleWebKit/537.36 (KHTML, like Gecko) \u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m     10\u001b[39m                   \u001b[33m\"\u001b[39m\u001b[33mChrome/98.0.4758.102 Safari/537.36\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m }\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import requests           # Realiza requisições HTTP com cabeçalho customizado\n",
    "from bs4 import BeautifulSoup  # Faz a análise do conteúdo HTML/XML\n",
    "import pandas as pd       # Manipulação e organização dos dados em DataFrame\n",
    "\n",
    "# Definição de um cabeçalho com User-Agent para simular um navegador real\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" \\\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \" \\\n",
    "                  \"Chrome/98.0.4758.102 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Realizando a requisição HTTP com o cabeçalho customizado\n",
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Encontrando os elementos que contêm as informações das vagas\n",
    "job_elements = soup.find_all('div', class_='card-content')\n",
    "\n",
    "# Inicializando listas para armazenar os dados extraídos\n",
    "titles = []\n",
    "companies = []\n",
    "locations = []\n",
    "\n",
    "# Iterando sobre os elementos para extrair os dados\n",
    "for job in job_elements:\n",
    "    title = job.find('h2', class_='title').get_text().strip()\n",
    "    company = job.find('h3', class_='company').get_text().strip() if job.find('h3', class_='company') else \"N/A\"\n",
    "    location = job.find('p', class_='location').get_text().strip() if job.find('p', class_='location') else \"N/A\"\n",
    "\n",
    "    titles.append(title)\n",
    "    companies.append(company)\n",
    "    locations.append(location)\n",
    "\n",
    "# Organização dos dados em um DataFrame\n",
    "df_jobs = pd.DataFrame({\n",
    "    \"Título\": titles,\n",
    "    \"Empresa\": companies,\n",
    "    \"Localização\": locations\n",
    "})\n",
    "\n",
    "# Exportação do DataFrame para um arquivo CSV (sem o índice)\n",
    "df_jobs.to_csv(\"fake_jobs.csv\", index=False)\n",
    "\n",
    "print(\"Dados extraídos e salvos no arquivo 'fake_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df7a54",
   "metadata": {},
   "source": [
    "## Robustez mínima (extensões futuras sem alterar o fluxo de hoje)\n",
    "\n",
    "- **Tratamento de erro:** `try/except` para requisição e parsing.  \n",
    "- **Controle de ritmo:** intervalo aleatório (`time.sleep`) para evitar sobrecarga.  \n",
    "- **Retry/backoff exponencial:** em caso de falha, tentar novamente com espera crescente.  \n",
    "- **Monitoramento de mudanças:** se o seletor quebrar, registrar a data e o seletor vigente para ajuste rápido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTq3jErk2L5R"
   },
   "source": [
    "# Aquela olhadinha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTemdPvz18Cg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "df_jobs = pd.read_csv(\"fake_jobs.csv\")\n",
    "print(\"Tabela de Vagas Extraídas\")\n",
    "print(tabulate(df_jobs.head(), headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c58f7",
   "metadata": {},
   "source": [
    "## Perguntas para reflexão/avaliação\n",
    "\n",
    "1. O seletor escolhido captura **apenas** os itens relevantes? O que faria para reduzir ruído?  \n",
    "2. Como verificar **cobertura** (quantos itens esperados vs. coletados)?  \n",
    "3. Se o site mudar a classe dos elementos amanhã, qual seria seu **plano de contingência**?  \n",
    "4. Há uma **API** disponível? O scraping continua justificável?  \n",
    "5. Quais campos coletados são suficientes para análises simples (frequências, séries, correlações)?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
